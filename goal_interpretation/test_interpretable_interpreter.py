#!/usr/bin/env python3
"""
InterPreTé›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•å¯è§£é‡Šç›®æ ‡è§£é‡Šå™¨çš„å„é¡¹åŠŸèƒ½
"""

import sys
import os
import unittest
from typing import Dict, Any, List
import tempfile
import json

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from interpretable_goal_interpreter import (
        InterpretableGoalInterpreter,
        InterPreTFeedbackLearner,
        PDDLDomainBuilder,
        FeedbackRecord,
        SymbolicPredicate
    )
    print("âœ… æˆåŠŸå¯¼å…¥InterPreTæ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿interpretable_goal_interpreter.pyåœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)

class TestInterpretableGoalInterpreter(unittest.TestCase):
    """InterPreTæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•ç±»"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.config = {
            'model_name': 'bert-base-uncased',
            'max_predicates': 50,
            'learning_rate': 0.001,
            'feedback_threshold': 0.8,
            'pddl_domain_name': 'test_domain'
        }
        self.interpreter = InterpretableGoalInterpreter(self.config)
        self.learner = InterPreTFeedbackLearner(self.config)
        self.domain_builder = PDDLDomainBuilder(self.config)
    
    def test_basic_interpretation(self):
        """æµ‹è¯•åŸºç¡€è§£é‡ŠåŠŸèƒ½"""
        print("\nðŸ§ª æµ‹è¯•1: åŸºç¡€è§£é‡ŠåŠŸèƒ½")
        
        test_goals = [
            "æ‹¿èµ·æ¯å­",
            "æ‰“å¼€é—¨",
            "èµ°åˆ°åŽ¨æˆ¿"
        ]
        
        for goal in test_goals:
            try:
                interpretation = self.interpreter.interpret_goal(goal)
                self.assertIsNotNone(interpretation)
                print(f"âœ… '{goal}' è§£é‡ŠæˆåŠŸ: {interpretation}")
            except Exception as e:
                self.fail(f"è§£é‡Š '{goal}' å¤±è´¥: {e}")
    
    def test_feedback_learning(self):
        """æµ‹è¯•åé¦ˆå­¦ä¹ åŠŸèƒ½"""
        print("\nðŸ§ª æµ‹è¯•2: åé¦ˆå­¦ä¹ åŠŸèƒ½")
        
        goal = "æŠŠçº¢è‰²çš„ä¹¦æ”¾åˆ°ä¹¦æž¶ä¸Š"
        
        # åˆ›å»ºæµ‹è¯•åé¦ˆ
        feedback = FeedbackRecord(
            goal=goal,
            user_feedback="åº”è¯¥å¼ºè°ƒé¢œè‰²å±žæ€§",
            corrected_predicate="is_red(book)",
            confidence=0.9
        )
        
        try:
            # å­¦ä¹ åé¦ˆ
            learned_predicate = self.learner.learn_from_feedback(feedback)
            self.assertIsNotNone(learned_predicate)
            print(f"âœ… åé¦ˆå­¦ä¹ æˆåŠŸ: {learned_predicate}")
            
            # éªŒè¯å­¦ä¹ æ•ˆæžœ
            self.assertTrue(hasattr(learned_predicate, 'name'))
            self.assertTrue(hasattr(learned_predicate, 'confidence'))
            
        except Exception as e:
            self.fail(f"åé¦ˆå­¦ä¹ å¤±è´¥: {e}")
    
    def test_pddl_domain_generation(self):
        """æµ‹è¯•PDDLåŸŸç”ŸæˆåŠŸèƒ½"""
        print("\nðŸ§ª æµ‹è¯•3: PDDLåŸŸç”ŸæˆåŠŸèƒ½")
        
        domain_info = {
            'name': 'test_domain',
            'types': ['robot', 'object', 'location'],
            'predicates': [
                ('at', ['robot', 'location']),
                ('holding', ['robot', 'object'])
            ],
            'actions': [
                {
                    'name': 'pickup',
                    'parameters': ['?r - robot', '?o - object'],
                    'precondition': '(at ?r ?l)',
                    'effect': '(holding ?r ?o)'
                }
            ]
        }
        
        try:
            # ç”ŸæˆPDDLåŸŸ
            pddl_domain = self.domain_builder.build_domain(domain_info)
            self.assertIsNotNone(pddl_domain)
            print(f"âœ… PDDLåŸŸç”ŸæˆæˆåŠŸ")
            
            # éªŒè¯PDDLè¯­æ³•
            is_valid = self.domain_builder.validate_domain(pddl_domain)
            self.assertTrue(is_valid, "PDDLåŸŸè¯­æ³•éªŒè¯å¤±è´¥")
            print(f"âœ… PDDLè¯­æ³•éªŒè¯é€šè¿‡")
            
        except Exception as e:
            self.fail(f"PDDLåŸŸç”Ÿæˆå¤±è´¥: {e}")
    
    def test_predicate_evolution(self):
        """æµ‹è¯•è°“è¯æ¼”åŒ–åŠŸèƒ½"""
        print("\nðŸ§ª æµ‹è¯•4: è°“è¯æ¼”åŒ–åŠŸèƒ½")
        
        try:
            # åˆ›å»ºåˆå§‹è°“è¯
            initial_predicates = [
                SymbolicPredicate("on", ["obj1", "obj2"], "obj1åœ¨obj2ä¸Š"),
                SymbolicPredicate("holding", ["agent", "obj"], "agentæ‹¿ç€obj")
            ]
            
            # æ¨¡æ‹Ÿæ¼”åŒ–
            new_predicate = SymbolicPredicate("is_red", ["obj"], "objæ˜¯çº¢è‰²çš„")
            evolved_predicates = initial_predicates + [new_predicate]
            
            # éªŒè¯æ¼”åŒ–ç»“æžœ
            self.assertEqual(len(evolved_predicates), 3)
            self.assertEqual(evolved_predicates[-1].name, "is_red")
            print(f"âœ… è°“è¯æ¼”åŒ–æˆåŠŸï¼Œæœ€ç»ˆè°“è¯æ•°é‡: {len(evolved_predicates)}")
            
        except Exception as e:
            self.fail(f"è°“è¯æ¼”åŒ–å¤±è´¥: {e}")
    
    def test_statistics_tracking(self):
        """æµ‹è¯•ç»Ÿè®¡è·Ÿè¸ªåŠŸèƒ½"""
        print("\nðŸ§ª æµ‹è¯•5: ç»Ÿè®¡è·Ÿè¸ªåŠŸèƒ½")
        
        try:
            # æ¨¡æ‹Ÿè§£é‡Šä»»åŠ¡
            test_goals = ["æµ‹è¯•ç›®æ ‡1", "æµ‹è¯•ç›®æ ‡2", "æµ‹è¯•ç›®æ ‡3"]
            
            for goal in test_goals:
                try:
                    interpretation = self.interpreter.interpret_goal(goal)
                    self.interpreter.update_statistics(goal, interpretation, success=True)
                except Exception:
                    self.interpreter.update_statistics(goal, None, success=False)
            
            # èŽ·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.interpreter.get_statistics()
            
            # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
            self.assertIn('total_tasks', stats)
            self.assertIn('successful_tasks', stats)
            self.assertIn('success_rate', stats)
            
            print(f"âœ… ç»Ÿè®¡è·Ÿè¸ªæˆåŠŸ")
            print(f"   æ€»ä»»åŠ¡æ•°: {stats['total_tasks']}")
            print(f"   æˆåŠŸçŽ‡: {stats['success_rate']:.2%}")
            
        except Exception as e:
            self.fail(f"ç»Ÿè®¡è·Ÿè¸ªå¤±è´¥: {e}")
    
    def test_save_load_functionality(self):
        """æµ‹è¯•ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½"""
        print("\nðŸ§ª æµ‹è¯•6: ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½")
        
        try:
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                temp_path = f.name
            
            # ä¿å­˜çŠ¶æ€
            self.interpreter.save_state(temp_path)
            self.assertTrue(os.path.exists(temp_path))
            print(f"âœ… çŠ¶æ€ä¿å­˜æˆåŠŸ")
            
            # åˆ›å»ºæ–°è§£é‡Šå™¨å¹¶åŠ è½½çŠ¶æ€
            new_interpreter = InterpretableGoalInterpreter()
            new_interpreter.load_state(temp_path)
            print(f"âœ… çŠ¶æ€åŠ è½½æˆåŠŸ")
            
            # éªŒè¯åŠ è½½æ•ˆæžœ
            test_goal = "æµ‹è¯•ç›®æ ‡"
            original_result = self.interpreter.interpret_goal(test_goal)
            loaded_result = new_interpreter.interpret_goal(test_goal)
            
            self.assertIsNotNone(original_result)
            self.assertIsNotNone(loaded_result)
            print(f"âœ… åŠ è½½éªŒè¯æˆåŠŸ")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)
            
        except Exception as e:
            self.fail(f"ä¿å­˜/åŠ è½½åŠŸèƒ½å¤±è´¥: {e}")

class TestInterPreTIntegration(unittest.TestCase):
    """InterPreTé›†æˆæµ‹è¯•ç±»"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.config = {
            'model_name': 'bert-base-uncased',
            'max_predicates': 20,
            'learning_rate': 0.001,
            'feedback_threshold': 0.7
        }
    
    def test_end_to_end_workflow(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹"""
        print("\nðŸ§ª æµ‹è¯•7: ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹")
        
        try:
            # åˆå§‹åŒ–ç»„ä»¶
            interpreter = InterpretableGoalInterpreter(self.config)
            learner = InterPreTFeedbackLearner(self.config)
            domain_builder = PDDLDomainBuilder(self.config)
            
            # æ¨¡æ‹Ÿå®Œæ•´å·¥ä½œæµç¨‹
            goal = "æŠŠçº¢è‰²çš„æ¯å­ä»Žæ¡Œå­ä¸Šæ‹¿åˆ°åŽ¨æˆ¿"
            
            # 1. åŸºç¡€è§£é‡Š
            interpretation = interpreter.interpret_goal(goal)
            self.assertIsNotNone(interpretation)
            print(f"âœ… æ­¥éª¤1: åŸºç¡€è§£é‡Šå®Œæˆ")
            
            # 2. æ·»åŠ åé¦ˆå­¦ä¹ 
            feedback = FeedbackRecord(
                goal=goal,
                user_feedback="éœ€è¦å¼ºè°ƒç§»åŠ¨åŠ¨ä½œ",
                corrected_predicate="move_to(cup, kitchen)",
                confidence=0.85
            )
            learned_predicate = learner.learn_from_feedback(feedback)
            self.assertIsNotNone(learned_predicate)
            print(f"âœ… æ­¥éª¤2: åé¦ˆå­¦ä¹ å®Œæˆ")
            
            # 3. ç”ŸæˆPDDLåŸŸ
            domain_info = {
                'name': 'kitchen_domain',
                'types': ['robot', 'object', 'location'],
                'predicates': [('at', ['robot', 'location'])],
                'actions': []
            }
            pddl_domain = domain_builder.build_domain(domain_info)
            self.assertIsNotNone(pddl_domain)
            print(f"âœ… æ­¥éª¤3: PDDLåŸŸç”Ÿæˆå®Œæˆ")
            
            # 4. æ›´æ–°ç»Ÿè®¡
            interpreter.update_statistics(goal, interpretation, success=True)
            stats = interpreter.get_statistics()
            self.assertGreater(stats['total_tasks'], 0)
            print(f"âœ… æ­¥éª¤4: ç»Ÿè®¡æ›´æ–°å®Œæˆ")
            
            print(f"âœ… ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            self.fail(f"ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹å¤±è´¥: {e}")
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\nðŸ§ª æµ‹è¯•8: é”™è¯¯å¤„ç†")
        
        try:
            interpreter = InterpretableGoalInterpreter(self.config)
            
            # æµ‹è¯•æ— æ•ˆè¾“å…¥
            with self.assertRaises(Exception):
                interpreter.interpret_goal("")  # ç©ºå­—ç¬¦ä¸²
            
            # æµ‹è¯•æ— æ•ˆåé¦ˆ
            learner = InterPreTFeedbackLearner(self.config)
            invalid_feedback = FeedbackRecord("", "", "", -1.0)  # æ— æ•ˆåé¦ˆ
            result = learner.learn_from_feedback(invalid_feedback)
            # åº”è¯¥è¿”å›žNoneæˆ–æŠ›å‡ºå¼‚å¸¸
            print(f"âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âš ï¸ é”™è¯¯å¤„ç†æµ‹è¯•éƒ¨åˆ†é€šè¿‡: {e}")

def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\nðŸš€ æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    config = {
        'model_name': 'bert-base-uncased',
        'max_predicates': 50,
        'learning_rate': 0.001
    }
    
    interpreter = InterpretableGoalInterpreter(config)
    
    # æµ‹è¯•è§£é‡Šæ€§èƒ½
    import time
    test_goals = [
        "æ‹¿èµ·æ¯å­",
        "æ‰“å¼€é—¨", 
        "èµ°åˆ°åŽ¨æˆ¿",
        "æ”¾ä¸‹ä¹¦",
        "å…³é—­çª—æˆ·"
    ] * 10  # é‡å¤10æ¬¡
    
    start_time = time.time()
    successful_interpretations = 0
    
    for goal in test_goals:
        try:
            interpretation = interpreter.interpret_goal(goal)
            successful_interpretations += 1
        except Exception:
            pass
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"ðŸ“Š æ€§èƒ½æµ‹è¯•ç»“æžœ:")
    print(f"   æ€»ä»»åŠ¡æ•°: {len(test_goals)}")
    print(f"   æˆåŠŸä»»åŠ¡æ•°: {successful_interpretations}")
    print(f"   æ€»è€—æ—¶: {total_time:.3f}ç§’")
    print(f"   å¹³å‡è€—æ—¶: {total_time/len(test_goals):.3f}ç§’/ä»»åŠ¡")
    print(f"   æˆåŠŸçŽ‡: {successful_interpretations/len(test_goals):.2%}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ðŸ§ª InterPreTå¯è§£é‡Šç›®æ ‡è§£é‡Šå™¨æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
    test_suite.addTest(unittest.makeSuite(TestInterpretableGoalInterpreter))
    
    # æ·»åŠ é›†æˆæµ‹è¯•
    test_suite.addTest(unittest.makeSuite(TestInterPreTIntegration))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    run_performance_tests()
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "=" * 60)
    print("ðŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"è¿è¡Œæµ‹è¯•: {result.testsRun}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"   - {test}: {traceback}")
    
    if result.errors:
        print("\nðŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"   - {test}: {traceback}")
    
    success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun
    
    if success_rate == 1.0:
        print("\nðŸŽ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼InterPreTé›†æˆæˆåŠŸï¼")
        print("ðŸš€ æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨InterPreTè¿›è¡Œå¼€å‘")
    elif success_rate >= 0.8:
        print(f"\nâœ… å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ ({success_rate:.1%})")
        print("âš ï¸ éƒ¨åˆ†åŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    else:
        print(f"\nâš ï¸ æµ‹è¯•é€šè¿‡çŽ‡è¾ƒä½Ž ({success_rate:.1%})")
        print("ðŸ”§ éœ€è¦æ£€æŸ¥å’Œä¿®å¤ç›¸å…³é—®é¢˜")
    
    return 0 if success_rate >= 0.8 else 1

if __name__ == "__main__":
    sys.exit(main())