#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆè¯„ä¼°æµ‹è¯•è„šæœ¬
æµ‹è¯•æ›´æ–°åçš„å­ç›®æ ‡åˆ†è§£è¯„ä¼°å’ŒåŠ¨ä½œåºåˆ—ç”Ÿæˆè¯„ä¼°åŠŸèƒ½
"""

import sys
import os
import json
from typing import Dict, List, Any
import traceback
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥é›†æˆæ¨¡å—
from goal_interpretation.goal_interpreter import GoalInterpreter
from subgoal_decomposition.subgoal_decomposer_integration import SubgoalDecomposerIntegration
from action_sequencing.action_sequencer_integration import ActionSequencerIntegration

class IntegrationEvaluationTester:
    """é›†æˆè¯„ä¼°æµ‹è¯•ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.goal_interpreter = GoalInterpreter()
        self.subgoal_integration = SubgoalDecomposerIntegration(config={
            'evaluate_results': True,
            'evaluator_config': {
                'enabled': True,
                'metrics': ['completeness', 'coherence', 'efficiency', 'correctness']
            }
        })
        self.action_integration = ActionSequencerIntegration(config={
            'evaluate_results': True,
            'evaluator_config': {
                'enabled': True,
                'metrics': ['sequence_quality', 'execution_feasibility', 'resource_efficiency']
            }
        })
        
        print("ğŸ”§ åˆå§‹åŒ–é›†æˆè¯„ä¼°æµ‹è¯•å™¨")
        print(f"   - ç›®æ ‡è§£é‡Šå™¨: {type(self.goal_interpreter).__name__}")
        print(f"   - å­ç›®æ ‡åˆ†è§£é›†æˆ: {type(self.subgoal_integration).__name__}")
        print(f"   - åŠ¨ä½œåºåˆ—é›†æˆ: {type(self.action_integration).__name__}")
    
    def test_goal_interpretation(self, natural_goal: str) -> Any:
        """æµ‹è¯•ç›®æ ‡è§£é‡Šæ¨¡å—"""
        print(f"\nğŸ“ æµ‹è¯•ç›®æ ‡è§£é‡Š: '{natural_goal}'")
        
        try:
            ltl_formula = self.goal_interpreter.interpret(natural_goal)
            print(f"âœ… ç›®æ ‡è§£é‡ŠæˆåŠŸ")
            print(f"   LTLå…¬å¼: {ltl_formula.formula}")
            return ltl_formula
            
        except Exception as e:
            print(f"âŒ ç›®æ ‡è§£é‡Šå¤±è´¥: {str(e)}")
            traceback.print_exc()
            return None
    
    def test_subgoal_decomposition_evaluation(self, ltl_formula: Any) -> Any:
        """æµ‹è¯•å­ç›®æ ‡åˆ†è§£è¯„ä¼°åŠŸèƒ½"""
        print(f"\nğŸ¯ æµ‹è¯•å­ç›®æ ‡åˆ†è§£è¯„ä¼°: '{ltl_formula.formula}'")
        
        try:
            # ä½¿ç”¨è¯„ä¼°åŠŸèƒ½è¿›è¡Œå­ç›®æ ‡åˆ†è§£
            decomposition_result = self.subgoal_integration.decompose_with_evaluation(
                ltl_formula, 
                evaluate=True
            )
            
            print(f"âœ… å­ç›®æ ‡åˆ†è§£è¯„ä¼°æˆåŠŸ")
            print(f"   å­ç›®æ ‡æ•°é‡: {len(decomposition_result.subgoals)}")
            print(f"   æ‰§è¡Œé¡ºåº: {decomposition_result.execution_order}")
            print(f"   æ€»æˆæœ¬: {decomposition_result.total_cost:.2f}")
            
            # æ£€æŸ¥è¯„ä¼°ç»“æœ
            if hasattr(decomposition_result, 'evaluation_results') and decomposition_result.evaluation_results:
                print(f"   è¯„ä¼°ç»“æœ:")
                for metric, value in decomposition_result.evaluation_results.items():
                    print(f"     - {metric}: {value}")
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°è¯„ä¼°ç»“æœ")
            
            return decomposition_result
            
        except Exception as e:
            print(f"âŒ å­ç›®æ ‡åˆ†è§£è¯„ä¼°å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return None
    
    def test_action_sequence_evaluation(self, decomposition_result: Any) -> Any:
        """æµ‹è¯•åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯„ä¼°åŠŸèƒ½"""
        print(f"\nâš¡ æµ‹è¯•åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯„ä¼°")
        
        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_params = {
                'decomposition_result': decomposition_result,
                'initial_state': {},
                'context': {
                    'goal': decomposition_result.original_goal,
                    'execution_environment': 'simulation'
                }
            }
            
            # ä½¿ç”¨è¯„ä¼°åŠŸèƒ½ç”ŸæˆåŠ¨ä½œåºåˆ—
            sequence_result = self.action_integration.sequence_actions_for_integration(
                request_params, 
                evaluate=True
            )
            
            print(f"âœ… åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯„ä¼°æˆåŠŸ")
            print(f"   åŠ¨ä½œæ•°é‡: {len(sequence_result.action_sequence)}")
            print(f"   ç½®ä¿¡åº¦: {sequence_result.confidence_score:.2f}")
            
            # æ£€æŸ¥è¯„ä¼°ç»“æœ
            if hasattr(sequence_result, 'evaluation_results') and sequence_result.evaluation_results:
                print(f"   è¯„ä¼°ç»“æœ:")
                for metric, value in sequence_result.evaluation_results.items():
                    print(f"     - {metric}: {value}")
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°è¯„ä¼°ç»“æœ")
            
            # æ˜¾ç¤ºåŠ¨ä½œåºåˆ—
            for i, action in enumerate(sequence_result.action_sequence):
                print(f"   åŠ¨ä½œ {i+1}: {action['name']} (ç±»å‹: {action['type']})")
            
            return sequence_result
            
        except Exception as e:
            print(f"âŒ åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯„ä¼°å¤±è´¥: {str(e)}")
            traceback.print_exc()
            return None
    
    def test_end_to_end_evaluation(self, natural_goal: str) -> Dict:
        """æµ‹è¯•ç«¯åˆ°ç«¯è¯„ä¼°æµç¨‹"""
        print(f"\nğŸš€ æµ‹è¯•ç«¯åˆ°ç«¯è¯„ä¼°: '{natural_goal}'")
        print("=" * 60)
        
        result = {
            "timestamp": datetime.now().isoformat(),
            "natural_goal": natural_goal,
            "success": False,
            "ltl_formula": None,
            "subgoal_evaluation": None,
            "action_evaluation": None,
            "errors": []
        }
        
        try:
            # æ­¥éª¤1: ç›®æ ‡è§£é‡Š
            ltl_formula = self.test_goal_interpretation(natural_goal)
            if not ltl_formula:
                result["errors"].append("ç›®æ ‡è§£é‡Šå¤±è´¥")
                return result
            
            result["ltl_formula"] = ltl_formula.formula
            
            # æ­¥éª¤2: å­ç›®æ ‡åˆ†è§£è¯„ä¼°
            decomposition_result = self.test_subgoal_decomposition_evaluation(ltl_formula)
            if not decomposition_result:
                result["errors"].append("å­ç›®æ ‡åˆ†è§£è¯„ä¼°å¤±è´¥")
                return result
            
            result["subgoal_evaluation"] = {
                "subgoal_count": len(decomposition_result.subgoals),
                "evaluation": getattr(decomposition_result, 'evaluation_results', None)
            }
            
            # æ­¥éª¤3: åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯„ä¼°
            action_result = self.test_action_sequence_evaluation(decomposition_result)
            if not action_result:
                result["errors"].append("åŠ¨ä½œåºåˆ—ç”Ÿæˆè¯„ä¼°å¤±è´¥")
                return result
            
            result["action_evaluation"] = {
                "action_count": len(action_result.action_sequence),
                "confidence": action_result.confidence_score,
                "evaluation": getattr(action_result, 'evaluation_results', None)
            }
            
            result["success"] = True
            print(f"\nğŸ‰ ç«¯åˆ°ç«¯è¯„ä¼°æµ‹è¯•æˆåŠŸ!")
            
        except Exception as e:
            result["errors"].append(f"è¯„ä¼°æµ‹è¯•å¼‚å¸¸: {str(e)}")
            print(f"\nâŒ ç«¯åˆ°ç«¯è¯„ä¼°æµ‹è¯•å¤±è´¥: {str(e)}")
            traceback.print_exc()
        
        return result
    
    def test_with_builtin_goals(self):
        """ä½¿ç”¨å†…ç½®ç›®æ ‡æµ‹è¯•è¯„ä¼°åŠŸèƒ½"""
        test_goals = [
            "å…ˆå»å¨æˆ¿æ‹¿æ¯å­ï¼Œç„¶ååˆ°å®¢å…å–æ°´",
            "å¦‚æœä¸‹é›¨ï¼Œå°±å¸¦ä¼å‡ºé—¨",
            "æ¯å¤©æ—©ä¸Šå…ˆåˆ·ç‰™ï¼Œç„¶åæ´—è„¸ï¼Œæœ€ååƒæ—©é¤",
            "å°†ä¹¦ä»ä¹¦æ¶æ‹¿åˆ°æ¡Œå­ä¸Šï¼Œç„¶åæ‰“å¼€ç”µè„‘"
        ]
        
        print(f"\nğŸ§ª ä½¿ç”¨å†…ç½®ç›®æ ‡è¿›è¡Œè¯„ä¼°æµ‹è¯•")
        print("=" * 60)
        
        results = []
        for i, goal in enumerate(test_goals, 1):
            print(f"\n--- æµ‹è¯• {i}/{len(test_goals)} ---\n")
            result = self.test_end_to_end_evaluation(goal)
            results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        successful_tests = sum(1 for r in results if r["success"])
        print(f"\nğŸ“Š è¯„ä¼°æµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {len(results)}")
        print(f"   æˆåŠŸæ•°: {successful_tests}")
        print(f"   å¤±è´¥æ•°: {len(results) - successful_tests}")
        print(f"   æˆåŠŸç‡: {successful_tests/len(results)*100:.1f}%")
        
        # ä¿å­˜ç»“æœ
        self._save_test_results(results)
        
        return results
    
    def _save_test_results(self, results: List[Dict]):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        try:
            result_dir = os.path.join(project_root, 'test_results')
            os.makedirs(result_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.join(result_dir, f"evaluation_test_results_{timestamp}.json")
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {str(e)}")
    
    def test_evaluation_metrics(self):
        """æµ‹è¯•ä¸åŒè¯„ä¼°æŒ‡æ ‡çš„æœ‰æ•ˆæ€§"""
        print(f"\nğŸ“ˆ æµ‹è¯•è¯„ä¼°æŒ‡æ ‡")
        print("=" * 60)
        
        # æµ‹è¯•åœºæ™¯1: ç®€å•ç›®æ ‡
        simple_goal = "åˆ°è¾¾å®¢å…"
        # æµ‹è¯•åœºæ™¯2: å¤æ‚ç›®æ ‡
        complex_goal = "å¦‚æœä¸‹é›¨ï¼Œå¸¦ä¼å»è¶…å¸‚ä¹°ç‰›å¥¶å’Œé¢åŒ…ï¼Œç„¶åå›å®¶"
        
        print(f"\nåœºæ™¯1: ç®€å•ç›®æ ‡ '{simple_goal}'")
        simple_result = self.test_end_to_end_evaluation(simple_goal)
        
        print(f"\nåœºæ™¯2: å¤æ‚ç›®æ ‡ '{complex_goal}'")
        complex_result = self.test_end_to_end_evaluation(complex_goal)
        
        # æ¯”è¾ƒç»“æœ
        print(f"\nğŸ” è¯„ä¼°æŒ‡æ ‡æ¯”è¾ƒ:")
        
        if simple_result.get("subgoal_evaluation") and complex_result.get("subgoal_evaluation"):
            simple_eval = simple_result["subgoal_evaluation"].get("evaluation", {})
            complex_eval = complex_result["subgoal_evaluation"].get("evaluation", {})
            
            print(f"\nå­ç›®æ ‡åˆ†è§£è¯„ä¼°æ¯”è¾ƒ:")
            metrics = set(simple_eval.keys()) | set(complex_eval.keys())
            for metric in metrics:
                simple_val = simple_eval.get(metric, "N/A")
                complex_val = complex_eval.get(metric, "N/A")
                print(f"   {metric}: ç®€å•ç›®æ ‡={simple_val}, å¤æ‚ç›®æ ‡={complex_val}")
        
        return {"simple_goal": simple_result, "complex_goal": complex_result}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ é›†æˆè¯„ä¼°åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = IntegrationEvaluationTester()
        
        # è¿è¡Œå†…ç½®ç›®æ ‡æµ‹è¯•
        print("\nç¬¬ä¸€éƒ¨åˆ†: åŸºæœ¬è¯„ä¼°åŠŸèƒ½æµ‹è¯•")
        results = tester.test_with_builtin_goals()
        
        # è¿è¡Œè¯„ä¼°æŒ‡æ ‡æµ‹è¯•
        print("\n\nç¬¬äºŒéƒ¨åˆ†: è¯„ä¼°æŒ‡æ ‡æœ‰æ•ˆæ€§æµ‹è¯•")
        metrics_results = tester.test_evaluation_metrics()
        
        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        traceback.print_exc()

if __name__ == "__main__":
    main()